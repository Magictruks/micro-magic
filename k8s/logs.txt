* 
* ==> Audit <==
* |------------|--------------------------|----------|--------------------|---------|---------------------|---------------------|
|  Command   |           Args           | Profile  |        User        | Version |     Start Time      |      End Time       |
|------------|--------------------------|----------|--------------------|---------|---------------------|---------------------|
| start      |                          | minikube | axelturchinim23000 | v1.32.0 | 19 Jan 24 11:12 CET |                     |
| start      | --driver docker          | minikube | axelturchinim23000 | v1.32.0 | 19 Jan 24 11:12 CET | 19 Jan 24 11:14 CET |
| stop       |                          | minikube | axelturchinim23000 | v1.32.0 | 21 Jan 24 17:47 CET | 21 Jan 24 17:47 CET |
| start      |                          | minikube | axelturchinim23000 | v1.32.0 | 21 Jan 24 17:48 CET |                     |
| stop       |                          | minikube | axelturchinim23000 | v1.32.0 | 21 Jan 24 17:48 CET | 21 Jan 24 17:48 CET |
| start      | --insecure-registry true | minikube | axelturchinim23000 | v1.32.0 | 21 Jan 24 17:49 CET | 21 Jan 24 17:49 CET |
| ip         |                          | minikube | axelturchinim23000 | v1.32.0 | 21 Jan 24 18:18 CET | 21 Jan 24 18:18 CET |
| dashboard  |                          | minikube | axelturchinim23000 | v1.32.0 | 22 Jan 24 16:38 CET |                     |
| addons     | enable metrics-server    | minikube | axelturchinim23000 | v1.32.0 | 22 Jan 24 16:39 CET | 22 Jan 24 16:39 CET |
| dashboard  |                          | minikube | axelturchinim23000 | v1.32.0 | 22 Jan 24 16:39 CET |                     |
| docker-env |                          | minikube | axelturchinim23000 | v1.32.0 | 22 Jan 24 16:45 CET | 22 Jan 24 16:45 CET |
| delete     |                          | minikube | axelturchinim23000 | v1.32.0 | 22 Jan 24 16:59 CET | 22 Jan 24 16:59 CET |
| start      |                          | minikube | axelturchinim23000 | v1.32.0 | 22 Jan 24 16:59 CET | 22 Jan 24 17:00 CET |
| delete     |                          | minikube | axelturchinim23000 | v1.32.0 | 22 Jan 24 17:20 CET | 22 Jan 24 17:21 CET |
| start      |                          | minikube | axelturchinim23000 | v1.32.0 | 22 Jan 24 17:38 CET | 22 Jan 24 17:39 CET |
| stop       |                          | minikube | axelturchinim23000 | v1.32.0 | 22 Jan 24 17:47 CET | 22 Jan 24 17:47 CET |
| start      | --insecure-registry true | minikube | axelturchinim23000 | v1.32.0 | 22 Jan 24 17:47 CET | 22 Jan 24 17:48 CET |
| service    |                          | minikube | axelturchinim23000 | v1.32.0 | 22 Jan 24 17:53 CET |                     |
| service    | auth-service --url       | minikube | axelturchinim23000 | v1.32.0 | 22 Jan 24 17:55 CET |                     |
| service    | auth-service --url       | minikube | axelturchinim23000 | v1.32.0 | 22 Jan 24 17:56 CET |                     |
| ip         |                          | minikube | axelturchinim23000 | v1.32.0 | 22 Jan 24 18:03 CET | 22 Jan 24 18:03 CET |
| service    | auth-service --url       | minikube | axelturchinim23000 | v1.32.0 | 22 Jan 24 18:08 CET |                     |
| service    | auth-service --url       | minikube | axelturchinim23000 | v1.32.0 | 22 Jan 24 18:14 CET |                     |
|------------|--------------------------|----------|--------------------|---------|---------------------|---------------------|

* 
* ==> Dernier d√©marrage <==
* Log file created at: 2024/01/22 17:47:47
Running on machine: MacBook-Pro-de-Axel
Binary: Built with gc go1.21.4 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0122 17:47:47.103068    3678 out.go:296] Setting OutFile to fd 1 ...
I0122 17:47:47.103253    3678 out.go:348] isatty.IsTerminal(1) = true
I0122 17:47:47.103255    3678 out.go:309] Setting ErrFile to fd 2...
I0122 17:47:47.103258    3678 out.go:348] isatty.IsTerminal(2) = true
I0122 17:47:47.103426    3678 root.go:338] Updating PATH: /Users/axelturchinim23000/.minikube/bin
I0122 17:47:47.104893    3678 out.go:303] Setting JSON to false
I0122 17:47:47.127563    3678 start.go:128] hostinfo: {"hostname":"MacBook-Pro-de-Axel.local","uptime":1511,"bootTime":1705940556,"procs":445,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"14.2.1","kernelVersion":"23.2.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"f0a9d17d-3965-50fb-9d51-81017eeeb8fb"}
W0122 17:47:47.127645    3678 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0122 17:47:47.134660    3678 out.go:177] üòÑ  minikube v1.32.0 sur Darwin 14.2.1 (arm64)
I0122 17:47:47.142868    3678 notify.go:220] Checking for updates...
I0122 17:47:47.143010    3678 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0122 17:47:47.143800    3678 driver.go:378] Setting default libvirt URI to qemu:///system
I0122 17:47:47.239840    3678 docker.go:122] docker version: linux-24.0.6:Docker Desktop 4.24.2 (124339)
I0122 17:47:47.239920    3678 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0122 17:47:47.513357    3678 info.go:266] docker info: {ID:ef6b5a9e-f6ee-4f63-bfe2-9e6f12911390 Containers:46 ContainersRunning:35 ContainersPaused:0 ContainersStopped:11 Images:19 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:163 OomKillDisable:false NGoroutines:166 SystemTime:2024-01-22 16:47:47.488644591 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:6.4.16-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:10 MemTotal:8232673280 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/axelturchinim23000/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:/Users/axelturchinim23000/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.22.0-desktop.2] map[Name:dev Path:/Users/axelturchinim23000/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/axelturchinim23000/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:/Users/axelturchinim23000/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.8] map[Name:sbom Path:/Users/axelturchinim23000/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/Users/axelturchinim23000/.docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/Users/axelturchinim23000/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.7]] Warnings:<nil>}}
I0122 17:47:47.519232    3678 out.go:177] ‚ú®  Utilisation du pilote docker bas√© sur le profil existant
I0122 17:47:47.522238    3678 start.go:298] selected driver: docker
I0122 17:47:47.522241    3678 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0122 17:47:47.522281    3678 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0122 17:47:47.522418    3678 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0122 17:47:47.610625    3678 info.go:266] docker info: {ID:ef6b5a9e-f6ee-4f63-bfe2-9e6f12911390 Containers:46 ContainersRunning:35 ContainersPaused:0 ContainersStopped:11 Images:19 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:163 OomKillDisable:false NGoroutines:166 SystemTime:2024-01-22 16:47:47.596166591 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:6.4.16-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:10 MemTotal:8232673280 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/axelturchinim23000/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:/Users/axelturchinim23000/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.22.0-desktop.2] map[Name:dev Path:/Users/axelturchinim23000/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/axelturchinim23000/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:/Users/axelturchinim23000/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.8] map[Name:sbom Path:/Users/axelturchinim23000/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/Users/axelturchinim23000/.docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/Users/axelturchinim23000/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.7]] Warnings:<nil>}}
I0122 17:47:47.611230    3678 cni.go:84] Creating CNI manager for ""
I0122 17:47:47.611250    3678 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0122 17:47:47.611254    3678 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0122 17:47:47.617261    3678 out.go:177] üëç  D√©marrage du noeud de plan de contr√¥le minikube dans le cluster minikube
I0122 17:47:47.620438    3678 cache.go:121] Beginning downloading kic base image for docker with docker
I0122 17:47:47.623241    3678 out.go:177] üöú  Extraction de l'image de base...
I0122 17:47:47.627370    3678 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0122 17:47:47.627392    3678 preload.go:148] Found local preload: /Users/axelturchinim23000/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-arm64.tar.lz4
I0122 17:47:47.627396    3678 cache.go:56] Caching tarball of preloaded images
I0122 17:47:47.627453    3678 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0122 17:47:47.627472    3678 preload.go:174] Found /Users/axelturchinim23000/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I0122 17:47:47.627476    3678 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0122 17:47:47.627757    3678 profile.go:148] Saving config to /Users/axelturchinim23000/.minikube/profiles/minikube/config.json ...
I0122 17:47:47.671263    3678 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0122 17:47:47.671285    3678 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0122 17:47:47.671320    3678 cache.go:194] Successfully downloaded all kic artifacts
I0122 17:47:47.671387    3678 start.go:365] acquiring machines lock for minikube: {Name:mk813fad0e12ab29e47e95b6108772158b3e8278 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0122 17:47:47.671463    3678 start.go:369] acquired machines lock for "minikube" in 62.042¬µs
I0122 17:47:47.671497    3678 start.go:96] Skipping create...Using existing machine configuration
I0122 17:47:47.671513    3678 fix.go:54] fixHost starting: 
I0122 17:47:47.671679    3678 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0122 17:47:47.708429    3678 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0122 17:47:47.708454    3678 fix.go:128] unexpected machine state, will restart: <nil>
I0122 17:47:47.713248    3678 out.go:177] üîÑ  Red√©marrage du docker container existant pour "minikube" ...
I0122 17:47:47.714412    3678 cli_runner.go:164] Run: docker start minikube
I0122 17:47:47.948940    3678 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0122 17:47:47.988805    3678 kic.go:430] container "minikube" state is running.
I0122 17:47:47.989337    3678 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0122 17:47:48.024311    3678 profile.go:148] Saving config to /Users/axelturchinim23000/.minikube/profiles/minikube/config.json ...
I0122 17:47:48.024869    3678 machine.go:88] provisioning docker machine ...
I0122 17:47:48.025288    3678 ubuntu.go:169] provisioning hostname "minikube"
I0122 17:47:48.025372    3678 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0122 17:47:48.068338    3678 main.go:141] libmachine: Using SSH client type: native
I0122 17:47:48.068913    3678 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102bfaf80] 0x102bfd6f0 <nil>  [] 0s} 127.0.0.1 49711 <nil> <nil>}
I0122 17:47:48.068919    3678 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0122 17:47:48.071596    3678 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0122 17:47:51.210672    3678 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0122 17:47:51.210797    3678 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0122 17:47:51.276833    3678 main.go:141] libmachine: Using SSH client type: native
I0122 17:47:51.277223    3678 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102bfaf80] 0x102bfd6f0 <nil>  [] 0s} 127.0.0.1 49711 <nil> <nil>}
I0122 17:47:51.277233    3678 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0122 17:47:51.390302    3678 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0122 17:47:51.390335    3678 ubuntu.go:175] set auth options {CertDir:/Users/axelturchinim23000/.minikube CaCertPath:/Users/axelturchinim23000/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/axelturchinim23000/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/axelturchinim23000/.minikube/machines/server.pem ServerKeyPath:/Users/axelturchinim23000/.minikube/machines/server-key.pem ClientKeyPath:/Users/axelturchinim23000/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/axelturchinim23000/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/axelturchinim23000/.minikube}
I0122 17:47:51.390371    3678 ubuntu.go:177] setting up certificates
I0122 17:47:51.390391    3678 provision.go:83] configureAuth start
I0122 17:47:51.390561    3678 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0122 17:47:51.462986    3678 provision.go:138] copyHostCerts
I0122 17:47:51.463133    3678 exec_runner.go:144] found /Users/axelturchinim23000/.minikube/ca.pem, removing ...
I0122 17:47:51.463140    3678 exec_runner.go:203] rm: /Users/axelturchinim23000/.minikube/ca.pem
I0122 17:47:51.463284    3678 exec_runner.go:151] cp: /Users/axelturchinim23000/.minikube/certs/ca.pem --> /Users/axelturchinim23000/.minikube/ca.pem (1111 bytes)
I0122 17:47:51.464953    3678 exec_runner.go:144] found /Users/axelturchinim23000/.minikube/cert.pem, removing ...
I0122 17:47:51.464962    3678 exec_runner.go:203] rm: /Users/axelturchinim23000/.minikube/cert.pem
I0122 17:47:51.465101    3678 exec_runner.go:151] cp: /Users/axelturchinim23000/.minikube/certs/cert.pem --> /Users/axelturchinim23000/.minikube/cert.pem (1151 bytes)
I0122 17:47:51.465496    3678 exec_runner.go:144] found /Users/axelturchinim23000/.minikube/key.pem, removing ...
I0122 17:47:51.465500    3678 exec_runner.go:203] rm: /Users/axelturchinim23000/.minikube/key.pem
I0122 17:47:51.465586    3678 exec_runner.go:151] cp: /Users/axelturchinim23000/.minikube/certs/key.pem --> /Users/axelturchinim23000/.minikube/key.pem (1679 bytes)
I0122 17:47:51.465905    3678 provision.go:112] generating server cert: /Users/axelturchinim23000/.minikube/machines/server.pem ca-key=/Users/axelturchinim23000/.minikube/certs/ca.pem private-key=/Users/axelturchinim23000/.minikube/certs/ca-key.pem org=axelturchinim23000.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0122 17:47:51.604343    3678 provision.go:172] copyRemoteCerts
I0122 17:47:51.604770    3678 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0122 17:47:51.604802    3678 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0122 17:47:51.645866    3678 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49711 SSHKeyPath:/Users/axelturchinim23000/.minikube/machines/minikube/id_rsa Username:docker}
I0122 17:47:51.744308    3678 ssh_runner.go:362] scp /Users/axelturchinim23000/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1111 bytes)
I0122 17:47:51.768779    3678 ssh_runner.go:362] scp /Users/axelturchinim23000/.minikube/machines/server.pem --> /etc/docker/server.pem (1233 bytes)
I0122 17:47:51.786993    3678 ssh_runner.go:362] scp /Users/axelturchinim23000/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0122 17:47:51.803409    3678 provision.go:86] duration metric: configureAuth took 413.011542ms
I0122 17:47:51.803416    3678 ubuntu.go:193] setting minikube options for container-runtime
I0122 17:47:51.803521    3678 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0122 17:47:51.803595    3678 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0122 17:47:51.846638    3678 main.go:141] libmachine: Using SSH client type: native
I0122 17:47:51.846970    3678 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102bfaf80] 0x102bfd6f0 <nil>  [] 0s} 127.0.0.1 49711 <nil> <nil>}
I0122 17:47:51.846975    3678 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0122 17:47:51.948700    3678 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0122 17:47:51.948711    3678 ubuntu.go:71] root file system type: overlay
I0122 17:47:51.948918    3678 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0122 17:47:51.949029    3678 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0122 17:47:51.999403    3678 main.go:141] libmachine: Using SSH client type: native
I0122 17:47:51.999706    3678 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102bfaf80] 0x102bfd6f0 <nil>  [] 0s} 127.0.0.1 49711 <nil> <nil>}
I0122 17:47:51.999742    3678 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0122 17:47:52.106280    3678 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0122 17:47:52.108550    3678 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0122 17:47:52.158348    3678 main.go:141] libmachine: Using SSH client type: native
I0122 17:47:52.158675    3678 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102bfaf80] 0x102bfd6f0 <nil>  [] 0s} 127.0.0.1 49711 <nil> <nil>}
I0122 17:47:52.158682    3678 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0122 17:47:52.260227    3678 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0122 17:47:52.260247    3678 machine.go:91] provisioned docker machine in 4.235361209s
I0122 17:47:52.260261    3678 start.go:300] post-start starting for "minikube" (driver="docker")
I0122 17:47:52.260278    3678 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0122 17:47:52.260547    3678 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0122 17:47:52.260689    3678 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0122 17:47:52.318513    3678 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49711 SSHKeyPath:/Users/axelturchinim23000/.minikube/machines/minikube/id_rsa Username:docker}
I0122 17:47:52.395729    3678 ssh_runner.go:195] Run: cat /etc/os-release
I0122 17:47:52.398454    3678 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0122 17:47:52.398490    3678 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0122 17:47:52.398502    3678 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0122 17:47:52.398507    3678 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0122 17:47:52.398515    3678 filesync.go:126] Scanning /Users/axelturchinim23000/.minikube/addons for local assets ...
I0122 17:47:52.398671    3678 filesync.go:126] Scanning /Users/axelturchinim23000/.minikube/files for local assets ...
I0122 17:47:52.398740    3678 start.go:303] post-start completed in 138.471208ms
I0122 17:47:52.398845    3678 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0122 17:47:52.398918    3678 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0122 17:47:52.453571    3678 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49711 SSHKeyPath:/Users/axelturchinim23000/.minikube/machines/minikube/id_rsa Username:docker}
I0122 17:47:52.530812    3678 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0122 17:47:52.534243    3678 fix.go:56] fixHost completed within 4.862733666s
I0122 17:47:52.534250    3678 start.go:83] releasing machines lock for "minikube", held for 4.862776125s
I0122 17:47:52.534302    3678 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0122 17:47:52.582909    3678 ssh_runner.go:195] Run: cat /version.json
I0122 17:47:52.582966    3678 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0122 17:47:52.583103    3678 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0122 17:47:52.583934    3678 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0122 17:47:52.627818    3678 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49711 SSHKeyPath:/Users/axelturchinim23000/.minikube/machines/minikube/id_rsa Username:docker}
I0122 17:47:52.628088    3678 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49711 SSHKeyPath:/Users/axelturchinim23000/.minikube/machines/minikube/id_rsa Username:docker}
I0122 17:47:52.850274    3678 ssh_runner.go:195] Run: systemctl --version
I0122 17:47:52.855393    3678 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0122 17:47:52.859005    3678 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0122 17:47:52.872736    3678 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0122 17:47:52.872890    3678 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0122 17:47:52.882380    3678 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0122 17:47:52.882403    3678 start.go:472] detecting cgroup driver to use...
I0122 17:47:52.882422    3678 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0122 17:47:52.883029    3678 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0122 17:47:52.894615    3678 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0122 17:47:52.903065    3678 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0122 17:47:52.910381    3678 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0122 17:47:52.910509    3678 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0122 17:47:52.917399    3678 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0122 17:47:52.923673    3678 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0122 17:47:52.929604    3678 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0122 17:47:52.935218    3678 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0122 17:47:52.940338    3678 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0122 17:47:52.945446    3678 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0122 17:47:52.950164    3678 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0122 17:47:52.954931    3678 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0122 17:47:52.993155    3678 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0122 17:47:53.050571    3678 start.go:472] detecting cgroup driver to use...
I0122 17:47:53.050588    3678 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0122 17:47:53.050677    3678 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0122 17:47:53.060682    3678 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0122 17:47:53.060741    3678 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0122 17:47:53.067964    3678 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0122 17:47:53.078276    3678 ssh_runner.go:195] Run: which cri-dockerd
I0122 17:47:53.080803    3678 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0122 17:47:53.086929    3678 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0122 17:47:53.097352    3678 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0122 17:47:53.159787    3678 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0122 17:47:53.196849    3678 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0122 17:47:53.196951    3678 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0122 17:47:53.207593    3678 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0122 17:47:53.267501    3678 ssh_runner.go:195] Run: sudo systemctl restart docker
I0122 17:47:53.423443    3678 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0122 17:47:53.461821    3678 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0122 17:47:53.499876    3678 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0122 17:47:53.537745    3678 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0122 17:47:53.577538    3678 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0122 17:47:53.595724    3678 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0122 17:47:53.633443    3678 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0122 17:47:53.686812    3678 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0122 17:47:53.687585    3678 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0122 17:47:53.690263    3678 start.go:540] Will wait 60s for crictl version
I0122 17:47:53.690322    3678 ssh_runner.go:195] Run: which crictl
I0122 17:47:53.692642    3678 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0122 17:47:53.723953    3678 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0122 17:47:53.724024    3678 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0122 17:47:53.737388    3678 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0122 17:47:53.754891    3678 out.go:204] üê≥  Pr√©paration de Kubernetes v1.28.3 sur Docker 24.0.7...
I0122 17:47:53.755355    3678 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0122 17:47:53.875603    3678 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0122 17:47:53.876058    3678 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0122 17:47:53.878697    3678 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0122 17:47:53.885057    3678 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0122 17:47:53.923215    3678 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0122 17:47:53.923251    3678 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0122 17:47:53.934041    3678 docker.go:671] Got preloaded images: -- stdout --
mysql:latest
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0122 17:47:53.934365    3678 docker.go:601] Images already preloaded, skipping extraction
I0122 17:47:53.934539    3678 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0122 17:47:53.944020    3678 docker.go:671] Got preloaded images: -- stdout --
mysql:latest
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0122 17:47:53.944027    3678 cache_images.go:84] Images are preloaded, skipping loading
I0122 17:47:53.944270    3678 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0122 17:47:53.980265    3678 cni.go:84] Creating CNI manager for ""
I0122 17:47:53.980271    3678 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0122 17:47:53.980705    3678 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0122 17:47:53.980719    3678 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0122 17:47:53.980802    3678 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0122 17:47:53.980855    3678 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0122 17:47:53.980921    3678 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0122 17:47:53.986394    3678 binaries.go:44] Found k8s binaries, skipping transfer
I0122 17:47:53.986447    3678 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0122 17:47:53.991320    3678 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0122 17:47:54.000334    3678 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0122 17:47:54.009834    3678 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0122 17:47:54.019723    3678 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0122 17:47:54.021915    3678 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0122 17:47:54.028953    3678 certs.go:56] Setting up /Users/axelturchinim23000/.minikube/profiles/minikube for IP: 192.168.49.2
I0122 17:47:54.029152    3678 certs.go:190] acquiring lock for shared ca certs: {Name:mkf256bc21d2d6aa1836fbdf0c28f9ba66597d39 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0122 17:47:54.029637    3678 certs.go:199] skipping minikubeCA CA generation: /Users/axelturchinim23000/.minikube/ca.key
I0122 17:47:54.029903    3678 certs.go:199] skipping proxyClientCA CA generation: /Users/axelturchinim23000/.minikube/proxy-client-ca.key
I0122 17:47:54.030296    3678 certs.go:315] skipping minikube-user signed cert generation: /Users/axelturchinim23000/.minikube/profiles/minikube/client.key
I0122 17:47:54.030656    3678 certs.go:315] skipping minikube signed cert generation: /Users/axelturchinim23000/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0122 17:47:54.030872    3678 certs.go:315] skipping aggregator signed cert generation: /Users/axelturchinim23000/.minikube/profiles/minikube/proxy-client.key
I0122 17:47:54.031358    3678 certs.go:437] found cert: /Users/axelturchinim23000/.minikube/certs/Users/axelturchinim23000/.minikube/certs/ca-key.pem (1675 bytes)
I0122 17:47:54.031405    3678 certs.go:437] found cert: /Users/axelturchinim23000/.minikube/certs/Users/axelturchinim23000/.minikube/certs/ca.pem (1111 bytes)
I0122 17:47:54.031439    3678 certs.go:437] found cert: /Users/axelturchinim23000/.minikube/certs/Users/axelturchinim23000/.minikube/certs/cert.pem (1151 bytes)
I0122 17:47:54.031483    3678 certs.go:437] found cert: /Users/axelturchinim23000/.minikube/certs/Users/axelturchinim23000/.minikube/certs/key.pem (1679 bytes)
I0122 17:47:54.033035    3678 ssh_runner.go:362] scp /Users/axelturchinim23000/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0122 17:47:54.045624    3678 ssh_runner.go:362] scp /Users/axelturchinim23000/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0122 17:47:54.058169    3678 ssh_runner.go:362] scp /Users/axelturchinim23000/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0122 17:47:54.071234    3678 ssh_runner.go:362] scp /Users/axelturchinim23000/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0122 17:47:54.084306    3678 ssh_runner.go:362] scp /Users/axelturchinim23000/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0122 17:47:54.096436    3678 ssh_runner.go:362] scp /Users/axelturchinim23000/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0122 17:47:54.109393    3678 ssh_runner.go:362] scp /Users/axelturchinim23000/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0122 17:47:54.122363    3678 ssh_runner.go:362] scp /Users/axelturchinim23000/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0122 17:47:54.134872    3678 ssh_runner.go:362] scp /Users/axelturchinim23000/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0122 17:47:54.147154    3678 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0122 17:47:54.156920    3678 ssh_runner.go:195] Run: openssl version
I0122 17:47:54.160320    3678 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0122 17:47:54.165965    3678 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0122 17:47:54.168191    3678 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Jan 19 10:13 /usr/share/ca-certificates/minikubeCA.pem
I0122 17:47:54.168231    3678 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0122 17:47:54.171945    3678 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0122 17:47:54.176921    3678 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0122 17:47:54.179139    3678 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0122 17:47:54.183055    3678 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0122 17:47:54.186709    3678 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0122 17:47:54.190351    3678 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0122 17:47:54.194179    3678 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0122 17:47:54.197613    3678 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0122 17:47:54.201373    3678 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0122 17:47:54.201469    3678 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0122 17:47:54.212573    3678 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0122 17:47:54.217737    3678 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0122 17:47:54.217766    3678 kubeadm.go:636] restartCluster start
I0122 17:47:54.217840    3678 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0122 17:47:54.222486    3678 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0122 17:47:54.222539    3678 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0122 17:47:54.284315    3678 kubeconfig.go:135] verify returned: extract IP: "minikube" does not appear in /Users/axelturchinim23000/.kube/config
I0122 17:47:54.284553    3678 kubeconfig.go:146] "minikube" context is missing from /Users/axelturchinim23000/.kube/config - will repair!
I0122 17:47:54.284899    3678 lock.go:35] WriteFile acquiring /Users/axelturchinim23000/.kube/config: {Name:mk179e5de6715baff6588a1297e06dfa2730f1b0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0122 17:47:54.293209    3678 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0122 17:47:54.299964    3678 api_server.go:166] Checking apiserver status ...
I0122 17:47:54.300004    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0122 17:47:54.305909    3678 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0122 17:47:54.305917    3678 api_server.go:166] Checking apiserver status ...
I0122 17:47:54.305956    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0122 17:47:54.313299    3678 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0122 17:47:54.814092    3678 api_server.go:166] Checking apiserver status ...
I0122 17:47:54.814481    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0122 17:47:54.835188    3678 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0122 17:47:55.314468    3678 api_server.go:166] Checking apiserver status ...
I0122 17:47:55.314939    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0122 17:47:55.339610    3678 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0122 17:47:55.814441    3678 api_server.go:166] Checking apiserver status ...
I0122 17:47:55.814804    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0122 17:47:55.835444    3678 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0122 17:47:56.314466    3678 api_server.go:166] Checking apiserver status ...
I0122 17:47:56.314761    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0122 17:47:56.322542    3678 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0122 17:47:56.814463    3678 api_server.go:166] Checking apiserver status ...
I0122 17:47:56.814807    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0122 17:47:56.838682    3678 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0122 17:47:57.314233    3678 api_server.go:166] Checking apiserver status ...
I0122 17:47:57.314627    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0122 17:47:57.345153    3678 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0122 17:47:57.813553    3678 api_server.go:166] Checking apiserver status ...
I0122 17:47:57.813913    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0122 17:47:57.836694    3678 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0122 17:47:58.314481    3678 api_server.go:166] Checking apiserver status ...
I0122 17:47:58.314843    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0122 17:47:58.335645    3678 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0122 17:47:58.813797    3678 api_server.go:166] Checking apiserver status ...
I0122 17:47:58.814024    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0122 17:47:58.838840    3678 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0122 17:47:59.313617    3678 api_server.go:166] Checking apiserver status ...
I0122 17:47:59.313900    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0122 17:47:59.336232    3678 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0122 17:47:59.813640    3678 api_server.go:166] Checking apiserver status ...
I0122 17:47:59.814002    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0122 17:47:59.837016    3678 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0122 17:48:00.313522    3678 api_server.go:166] Checking apiserver status ...
I0122 17:48:00.313817    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0122 17:48:00.338859    3678 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0122 17:48:00.814216    3678 api_server.go:166] Checking apiserver status ...
I0122 17:48:00.814600    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0122 17:48:00.839603    3678 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0122 17:48:01.313921    3678 api_server.go:166] Checking apiserver status ...
I0122 17:48:01.314413    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0122 17:48:01.335681    3678 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0122 17:48:01.814425    3678 api_server.go:166] Checking apiserver status ...
I0122 17:48:01.814891    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0122 17:48:01.834241    3678 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0122 17:48:02.314412    3678 api_server.go:166] Checking apiserver status ...
I0122 17:48:02.314803    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0122 17:48:02.336832    3678 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0122 17:48:02.814436    3678 api_server.go:166] Checking apiserver status ...
I0122 17:48:02.814807    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0122 17:48:02.842466    3678 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0122 17:48:03.314360    3678 api_server.go:166] Checking apiserver status ...
I0122 17:48:03.314928    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0122 17:48:03.339424    3678 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0122 17:48:03.814388    3678 api_server.go:166] Checking apiserver status ...
I0122 17:48:03.814800    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0122 17:48:03.842249    3678 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0122 17:48:04.301581    3678 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0122 17:48:04.301621    3678 kubeadm.go:1128] stopping kube-system containers ...
I0122 17:48:04.301890    3678 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0122 17:48:04.343817    3678 docker.go:469] Stopping containers: [6dd375b50dc6 49aee3bbddc5 c3c394540e06 1963668e8903 627c6923118f c3274a7dfc91 47dc11654088 95c8d5174e5d 1ee18d8071a9 b23be9d2fd5f ed734d64ec43 5bb469500b6f 065509fb18d7 745dad8d6186 c9bee2941b5c]
I0122 17:48:04.344013    3678 ssh_runner.go:195] Run: docker stop 6dd375b50dc6 49aee3bbddc5 c3c394540e06 1963668e8903 627c6923118f c3274a7dfc91 47dc11654088 95c8d5174e5d 1ee18d8071a9 b23be9d2fd5f ed734d64ec43 5bb469500b6f 065509fb18d7 745dad8d6186 c9bee2941b5c
I0122 17:48:04.366407    3678 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0122 17:48:04.377165    3678 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0122 17:48:04.383225    3678 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Jan 22 16:39 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Jan 22 16:39 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Jan 22 16:39 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Jan 22 16:39 /etc/kubernetes/scheduler.conf

I0122 17:48:04.383288    3678 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0122 17:48:04.389088    3678 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0122 17:48:04.394138    3678 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0122 17:48:04.398911    3678 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0122 17:48:04.398953    3678 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0122 17:48:04.404396    3678 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0122 17:48:04.409160    3678 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0122 17:48:04.409230    3678 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0122 17:48:04.414195    3678 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0122 17:48:04.418949    3678 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0122 17:48:04.418973    3678 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0122 17:48:04.453987    3678 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0122 17:48:04.810395    3678 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0122 17:48:04.898033    3678 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0122 17:48:04.927803    3678 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0122 17:48:04.955641    3678 api_server.go:52] waiting for apiserver process to appear ...
I0122 17:48:04.955765    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0122 17:48:04.962134    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0122 17:48:05.471619    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0122 17:48:05.972068    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0122 17:48:05.979740    3678 api_server.go:72] duration metric: took 1.024098209s to wait for apiserver process to appear ...
I0122 17:48:05.979751    3678 api_server.go:88] waiting for apiserver healthz status ...
I0122 17:48:05.980592    3678 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49710/healthz ...
I0122 17:48:07.297185    3678 api_server.go:279] https://127.0.0.1:49710/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0122 17:48:07.297202    3678 api_server.go:103] status: https://127.0.0.1:49710/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0122 17:48:07.297211    3678 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49710/healthz ...
I0122 17:48:07.357422    3678 api_server.go:279] https://127.0.0.1:49710/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0122 17:48:07.357440    3678 api_server.go:103] status: https://127.0.0.1:49710/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0122 17:48:07.858185    3678 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49710/healthz ...
I0122 17:48:07.875764    3678 api_server.go:279] https://127.0.0.1:49710/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0122 17:48:07.875816    3678 api_server.go:103] status: https://127.0.0.1:49710/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0122 17:48:08.358573    3678 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49710/healthz ...
I0122 17:48:08.364228    3678 api_server.go:279] https://127.0.0.1:49710/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0122 17:48:08.364258    3678 api_server.go:103] status: https://127.0.0.1:49710/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0122 17:48:08.858190    3678 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49710/healthz ...
I0122 17:48:08.862829    3678 api_server.go:279] https://127.0.0.1:49710/healthz returned 200:
ok
I0122 17:48:08.871637    3678 api_server.go:141] control plane version: v1.28.3
I0122 17:48:08.871651    3678 api_server.go:131] duration metric: took 2.891891041s to wait for apiserver health ...
I0122 17:48:08.871661    3678 cni.go:84] Creating CNI manager for ""
I0122 17:48:08.871678    3678 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0122 17:48:08.875712    3678 out.go:177] üîó  Configuration de bridge CNI (Container Networking Interface)...
I0122 17:48:08.878799    3678 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0122 17:48:08.885883    3678 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0122 17:48:08.896072    3678 system_pods.go:43] waiting for kube-system pods to appear ...
I0122 17:48:08.903536    3678 system_pods.go:59] 7 kube-system pods found
I0122 17:48:08.903555    3678 system_pods.go:61] "coredns-5dd5756b68-5czpx" [cedea4b8-c75f-41fd-b923-ac51c47b91c6] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0122 17:48:08.903562    3678 system_pods.go:61] "etcd-minikube" [3babd2ea-7530-461a-b31f-88f2e89d50e6] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0122 17:48:08.903569    3678 system_pods.go:61] "kube-apiserver-minikube" [b1d3ece8-7c79-4ba4-9b12-46a9cde98d89] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0122 17:48:08.903575    3678 system_pods.go:61] "kube-controller-manager-minikube" [6a5ef816-6cf1-420a-85ea-31a36d5c0829] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0122 17:48:08.903580    3678 system_pods.go:61] "kube-proxy-79dxz" [561e88df-ee5e-4dde-b670-7e65c7255277] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0122 17:48:08.903586    3678 system_pods.go:61] "kube-scheduler-minikube" [49aeb010-48ad-45ae-b382-ec825716a01d] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0122 17:48:08.903591    3678 system_pods.go:61] "storage-provisioner" [7d929465-23b1-43ce-b818-a86581219224] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0122 17:48:08.903595    3678 system_pods.go:74] duration metric: took 7.5145ms to wait for pod list to return data ...
I0122 17:48:08.903600    3678 node_conditions.go:102] verifying NodePressure condition ...
I0122 17:48:08.905663    3678 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0122 17:48:08.905672    3678 node_conditions.go:123] node cpu capacity is 10
I0122 17:48:08.905680    3678 node_conditions.go:105] duration metric: took 2.076292ms to run NodePressure ...
I0122 17:48:08.905690    3678 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0122 17:48:08.984384    3678 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0122 17:48:08.989237    3678 ops.go:34] apiserver oom_adj: -16
I0122 17:48:08.989245    3678 kubeadm.go:640] restartCluster took 14.7714515s
I0122 17:48:08.989251    3678 kubeadm.go:406] StartCluster complete in 14.787857375s
I0122 17:48:08.989267    3678 settings.go:142] acquiring lock: {Name:mk13010e02ef37f49710cbc0030832769ad4073a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0122 17:48:08.989393    3678 settings.go:150] Updating kubeconfig:  /Users/axelturchinim23000/.kube/config
I0122 17:48:08.990736    3678 lock.go:35] WriteFile acquiring /Users/axelturchinim23000/.kube/config: {Name:mk179e5de6715baff6588a1297e06dfa2730f1b0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0122 17:48:08.991107    3678 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0122 17:48:08.991457    3678 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0122 17:48:08.991375    3678 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0122 17:48:08.991609    3678 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0122 17:48:08.991621    3678 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0122 17:48:08.991625    3678 addons.go:240] addon storage-provisioner should already be in state true
I0122 17:48:08.991644    3678 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0122 17:48:08.991827    3678 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0122 17:48:08.991996    3678 host.go:66] Checking if "minikube" exists ...
I0122 17:48:08.992465    3678 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0122 17:48:08.992580    3678 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0122 17:48:08.995470    3678 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0122 17:48:08.995533    3678 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0122 17:48:08.998639    3678 out.go:177] üîé  V√©rification des composants Kubernetes...
I0122 17:48:09.003719    3678 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0122 17:48:09.054256    3678 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0122 17:48:09.054294    3678 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0122 17:48:09.091494    3678 out.go:177]     ‚ñ™ Utilisation de l'image gcr.io/k8s-minikube/storage-provisioner:v5
I0122 17:48:09.090953    3678 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0122 17:48:09.091542    3678 addons.go:240] addon default-storageclass should already be in state true
I0122 17:48:09.093740    3678 host.go:66] Checking if "minikube" exists ...
I0122 17:48:09.093761    3678 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0122 17:48:09.093767    3678 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0122 17:48:09.093821    3678 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0122 17:48:09.094670    3678 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0122 17:48:09.098708    3678 api_server.go:52] waiting for apiserver process to appear ...
I0122 17:48:09.098809    3678 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0122 17:48:09.106739    3678 api_server.go:72] duration metric: took 111.176542ms to wait for apiserver process to appear ...
I0122 17:48:09.106752    3678 api_server.go:88] waiting for apiserver healthz status ...
I0122 17:48:09.106769    3678 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49710/healthz ...
I0122 17:48:09.110462    3678 api_server.go:279] https://127.0.0.1:49710/healthz returned 200:
ok
I0122 17:48:09.111408    3678 api_server.go:141] control plane version: v1.28.3
I0122 17:48:09.111411    3678 api_server.go:131] duration metric: took 4.657375ms to wait for apiserver health ...
I0122 17:48:09.111414    3678 system_pods.go:43] waiting for kube-system pods to appear ...
I0122 17:48:09.115493    3678 system_pods.go:59] 7 kube-system pods found
I0122 17:48:09.115504    3678 system_pods.go:61] "coredns-5dd5756b68-5czpx" [cedea4b8-c75f-41fd-b923-ac51c47b91c6] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0122 17:48:09.115509    3678 system_pods.go:61] "etcd-minikube" [3babd2ea-7530-461a-b31f-88f2e89d50e6] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0122 17:48:09.115512    3678 system_pods.go:61] "kube-apiserver-minikube" [b1d3ece8-7c79-4ba4-9b12-46a9cde98d89] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0122 17:48:09.115515    3678 system_pods.go:61] "kube-controller-manager-minikube" [6a5ef816-6cf1-420a-85ea-31a36d5c0829] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0122 17:48:09.115523    3678 system_pods.go:61] "kube-proxy-79dxz" [561e88df-ee5e-4dde-b670-7e65c7255277] Running
I0122 17:48:09.115525    3678 system_pods.go:61] "kube-scheduler-minikube" [49aeb010-48ad-45ae-b382-ec825716a01d] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0122 17:48:09.115527    3678 system_pods.go:61] "storage-provisioner" [7d929465-23b1-43ce-b818-a86581219224] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0122 17:48:09.115529    3678 system_pods.go:74] duration metric: took 4.113042ms to wait for pod list to return data ...
I0122 17:48:09.115532    3678 kubeadm.go:581] duration metric: took 119.970833ms to wait for : map[apiserver:true system_pods:true] ...
I0122 17:48:09.115538    3678 node_conditions.go:102] verifying NodePressure condition ...
I0122 17:48:09.116988    3678 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0122 17:48:09.116991    3678 node_conditions.go:123] node cpu capacity is 10
I0122 17:48:09.117004    3678 node_conditions.go:105] duration metric: took 1.465083ms to run NodePressure ...
I0122 17:48:09.117009    3678 start.go:228] waiting for startup goroutines ...
I0122 17:48:09.132891    3678 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49711 SSHKeyPath:/Users/axelturchinim23000/.minikube/machines/minikube/id_rsa Username:docker}
I0122 17:48:09.132938    3678 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0122 17:48:09.132943    3678 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0122 17:48:09.132990    3678 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0122 17:48:09.167721    3678 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49711 SSHKeyPath:/Users/axelturchinim23000/.minikube/machines/minikube/id_rsa Username:docker}
I0122 17:48:09.212197    3678 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0122 17:48:09.245625    3678 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0122 17:48:09.592487    3678 out.go:177] üåü  Modules activ√©s: storage-provisioner, default-storageclass
I0122 17:48:09.598466    3678 addons.go:502] enable addons completed in 607.071791ms: enabled=[storage-provisioner default-storageclass]
I0122 17:48:09.598533    3678 start.go:233] waiting for cluster config update ...
I0122 17:48:09.598550    3678 start.go:242] writing updated cluster config ...
I0122 17:48:09.600833    3678 ssh_runner.go:195] Run: rm -f paused
I0122 17:48:09.765789    3678 start.go:600] kubectl: 1.29.1, cluster: 1.28.3 (minor skew: 1)
I0122 17:48:09.769449    3678 out.go:177] üèÑ  Termin√© ! kubectl est maintenant configur√© pour utiliser "minikube" cluster et espace de noms "default" par d√©faut.

* 
* ==> Docker <==
* Jan 22 16:47:53 minikube cri-dockerd[1031]: time="2024-01-22T16:47:53Z" level=info msg="Setting cgroupDriver cgroupfs"
Jan 22 16:47:53 minikube cri-dockerd[1031]: time="2024-01-22T16:47:53Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jan 22 16:47:53 minikube cri-dockerd[1031]: time="2024-01-22T16:47:53Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jan 22 16:47:53 minikube cri-dockerd[1031]: time="2024-01-22T16:47:53Z" level=info msg="Start cri-dockerd grpc backend"
Jan 22 16:47:53 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jan 22 16:48:04 minikube cri-dockerd[1031]: time="2024-01-22T16:48:04Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-5czpx_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c3c394540e069edffb5d9ab01162e94029be4ff9c52b89cfe309fd1ccb47d556\""
Jan 22 16:48:04 minikube cri-dockerd[1031]: time="2024-01-22T16:48:04Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"mysql-user-deployment-6dc8457659-d5r5j_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"021519c6d865e9b3c3d555fb4ad4f7115db042213a8fb689b515aa1393424b95\""
Jan 22 16:48:04 minikube cri-dockerd[1031]: time="2024-01-22T16:48:04Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"auth-deployment-54f94f4bb5-crrkg_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"690b2ad0682aee81ddc3c98a04904d2a4b85895b73987a76dc59d9bdcb7eb597\""
Jan 22 16:48:05 minikube cri-dockerd[1031]: time="2024-01-22T16:48:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4cb5b41985bde80c58b9f587d9a4c398ca6d2a44ac77c4fe9b06e6b3ad6b15a3/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 22 16:48:05 minikube cri-dockerd[1031]: time="2024-01-22T16:48:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3898b8ab1eb21f92c11453c02a871ef84d80845f80a60e8a3b3161ff4da6e3b3/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 22 16:48:05 minikube cri-dockerd[1031]: time="2024-01-22T16:48:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/50ebf459a33891608a94084d7892030e1c160bb3f9877c3563aba4543fc05e6c/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 22 16:48:05 minikube cri-dockerd[1031]: time="2024-01-22T16:48:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/628feb2bae19ef4769029508a8ae2d621211996ba8654c586be3ef5f436fc9f0/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 22 16:48:07 minikube cri-dockerd[1031]: time="2024-01-22T16:48:07Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jan 22 16:48:08 minikube cri-dockerd[1031]: time="2024-01-22T16:48:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1940da13447dbb100faf0e658e6a33883b26abee52da22687d8ecff3ee7a7543/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 22 16:48:08 minikube cri-dockerd[1031]: time="2024-01-22T16:48:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f2cf412d7bfd2835289cecfcebe44af95967f330fb1967a2f88009b864118d61/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 22 16:48:08 minikube cri-dockerd[1031]: time="2024-01-22T16:48:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c4ac488fce88be43e8fe55153287898553ff06e6b738a134e8537af69cd2e8f0/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 22 16:48:08 minikube cri-dockerd[1031]: time="2024-01-22T16:48:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/23bab7158437afc75c9f6bc67944ac96fd3c22421f68fe6335332ab204ab4953/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 22 16:48:08 minikube cri-dockerd[1031]: time="2024-01-22T16:48:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3bca707542a5a8a5289198c7fad24e389a1fc71e493977e69b3a669e8ef85f4d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 22 16:48:08 minikube dockerd[803]: time="2024-01-22T16:48:08.665781712Z" level=info msg="ignoring event" container=8f67926b73171e55c994425bbedcfe642572db79e60908276917fa04fe80383d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 22 16:48:09 minikube cri-dockerd[1031]: time="2024-01-22T16:48:09Z" level=info msg="Stop pulling image mysql:latest: Status: Image is up to date for mysql:latest"
Jan 22 16:48:09 minikube dockerd[803]: time="2024-01-22T16:48:09.695289338Z" level=warning msg="Error getting v2 registry: Get \"https://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:48:09 minikube dockerd[803]: time="2024-01-22T16:48:09.695319629Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:48:09 minikube dockerd[803]: time="2024-01-22T16:48:09.695556379Z" level=warning msg="Error getting v2 registry: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:48:09 minikube dockerd[803]: time="2024-01-22T16:48:09.695567671Z" level=info msg="Attempting next endpoint for pull after error: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:48:09 minikube dockerd[803]: time="2024-01-22T16:48:09.696330713Z" level=error msg="Handler for POST /v1.42/images/create returned error: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:48:20 minikube dockerd[803]: time="2024-01-22T16:48:20.979111718Z" level=warning msg="Error getting v2 registry: Get \"https://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:48:20 minikube dockerd[803]: time="2024-01-22T16:48:20.979135968Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:48:20 minikube dockerd[803]: time="2024-01-22T16:48:20.979398218Z" level=warning msg="Error getting v2 registry: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:48:20 minikube dockerd[803]: time="2024-01-22T16:48:20.979415218Z" level=info msg="Attempting next endpoint for pull after error: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:48:20 minikube dockerd[803]: time="2024-01-22T16:48:20.980639260Z" level=error msg="Handler for POST /v1.42/images/create returned error: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:48:36 minikube dockerd[803]: time="2024-01-22T16:48:36.778108170Z" level=info msg="ignoring event" container=3bca707542a5a8a5289198c7fad24e389a1fc71e493977e69b3a669e8ef85f4d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 22 16:48:37 minikube cri-dockerd[1031]: time="2024-01-22T16:48:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/126f340c344b02bdcd58544a7f128e8e3dae9b3ebb998044c0a1133c606630d1/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 22 16:48:37 minikube dockerd[803]: time="2024-01-22T16:48:37.145255086Z" level=warning msg="Error getting v2 registry: Get \"https://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:48:37 minikube dockerd[803]: time="2024-01-22T16:48:37.145285711Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:48:37 minikube dockerd[803]: time="2024-01-22T16:48:37.145505795Z" level=warning msg="Error getting v2 registry: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:48:37 minikube dockerd[803]: time="2024-01-22T16:48:37.145520211Z" level=info msg="Attempting next endpoint for pull after error: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:48:37 minikube dockerd[803]: time="2024-01-22T16:48:37.146078461Z" level=error msg="Handler for POST /v1.42/images/create returned error: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:48:49 minikube dockerd[803]: time="2024-01-22T16:48:49.988348551Z" level=warning msg="Error getting v2 registry: Get \"https://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:48:49 minikube dockerd[803]: time="2024-01-22T16:48:49.988438092Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:48:49 minikube dockerd[803]: time="2024-01-22T16:48:49.989499134Z" level=warning msg="Error getting v2 registry: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:48:49 minikube dockerd[803]: time="2024-01-22T16:48:49.989541051Z" level=info msg="Attempting next endpoint for pull after error: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:48:49 minikube dockerd[803]: time="2024-01-22T16:48:49.992245426Z" level=error msg="Handler for POST /v1.42/images/create returned error: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:49:16 minikube dockerd[803]: time="2024-01-22T16:49:16.993090174Z" level=warning msg="Error getting v2 registry: Get \"https://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:49:16 minikube dockerd[803]: time="2024-01-22T16:49:16.993246008Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:49:16 minikube dockerd[803]: time="2024-01-22T16:49:16.994022591Z" level=warning msg="Error getting v2 registry: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:49:16 minikube dockerd[803]: time="2024-01-22T16:49:16.994058841Z" level=info msg="Attempting next endpoint for pull after error: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:49:17 minikube dockerd[803]: time="2024-01-22T16:49:17.001865133Z" level=error msg="Handler for POST /v1.42/images/create returned error: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:49:59 minikube dockerd[803]: time="2024-01-22T16:49:59.986084083Z" level=warning msg="Error getting v2 registry: Get \"https://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:49:59 minikube dockerd[803]: time="2024-01-22T16:49:59.986183250Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:49:59 minikube dockerd[803]: time="2024-01-22T16:49:59.986882500Z" level=warning msg="Error getting v2 registry: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:49:59 minikube dockerd[803]: time="2024-01-22T16:49:59.986935750Z" level=info msg="Attempting next endpoint for pull after error: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:49:59 minikube dockerd[803]: time="2024-01-22T16:49:59.989162250Z" level=error msg="Handler for POST /v1.42/images/create returned error: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:51:31 minikube dockerd[803]: time="2024-01-22T16:51:31.991137584Z" level=warning msg="Error getting v2 registry: Get \"https://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:51:31 minikube dockerd[803]: time="2024-01-22T16:51:31.991324167Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:51:31 minikube dockerd[803]: time="2024-01-22T16:51:31.992087376Z" level=warning msg="Error getting v2 registry: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:51:31 minikube dockerd[803]: time="2024-01-22T16:51:31.992145792Z" level=info msg="Attempting next endpoint for pull after error: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:51:32 minikube dockerd[803]: time="2024-01-22T16:51:32.002568751Z" level=error msg="Handler for POST /v1.42/images/create returned error: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused"
Jan 22 16:51:37 minikube cri-dockerd[1031]: time="2024-01-22T16:51:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1f38fc014f8573bf014e4ca2798e3e7044a7ca58ae3e485dc4757a010822f33e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 22 16:51:45 minikube cri-dockerd[1031]: time="2024-01-22T16:51:45Z" level=info msg="Stop pulling image magictruks/spring-auth-service:latest: Status: Downloaded newer image for magictruks/spring-auth-service:latest"
Jan 22 16:51:46 minikube dockerd[803]: time="2024-01-22T16:51:46.350371007Z" level=info msg="ignoring event" container=126f340c344b02bdcd58544a7f128e8e3dae9b3ebb998044c0a1133c606630d1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                    CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
e5053246440cf       magictruks/spring-auth-service@sha256:d9d937f20b95b337b1eeab06f431cd6a63d9b70f274d5fe8a1784d2be2e2785c   23 minutes ago      Running             auth                      0                   1f38fc014f857       auth-deployment-97d67ff76-64xkq
9509d491d3b76       ba04bb24b9575                                                                                            26 minutes ago      Running             storage-provisioner       3                   f2cf412d7bfd2       storage-provisioner
704bb2b9d5a3b       mysql@sha256:d7c20c5ba268c558f4fac62977f8c7125bde0630ff8946b08dde44135ef40df3                            26 minutes ago      Running             mysql-user-db             1                   23bab7158437a       mysql-user-deployment-6dc8457659-d5r5j
df0f905c80f3f       97e04611ad434                                                                                            26 minutes ago      Running             coredns                   1                   c4ac488fce88b       coredns-5dd5756b68-5czpx
a2d0ee08b8f70       a5dd5cdd6d3ef                                                                                            26 minutes ago      Running             kube-proxy                1                   1940da13447db       kube-proxy-79dxz
8f67926b73171       ba04bb24b9575                                                                                            26 minutes ago      Exited              storage-provisioner       2                   f2cf412d7bfd2       storage-provisioner
43ba01a57049e       42a4e73724daa                                                                                            26 minutes ago      Running             kube-scheduler            1                   628feb2bae19e       kube-scheduler-minikube
39a65e87451ba       8276439b4f237                                                                                            26 minutes ago      Running             kube-controller-manager   1                   50ebf459a3389       kube-controller-manager-minikube
6046d13cc483c       537e9a59ee2fd                                                                                            26 minutes ago      Running             kube-apiserver            1                   3898b8ab1eb21       kube-apiserver-minikube
18fb1813b461f       9cdd6470f48c8                                                                                            26 minutes ago      Running             etcd                      1                   4cb5b41985bde       etcd-minikube
f617b496a594d       mysql@sha256:d7c20c5ba268c558f4fac62977f8c7125bde0630ff8946b08dde44135ef40df3                            34 minutes ago      Exited              mysql-user-db             0                   021519c6d865e       mysql-user-deployment-6dc8457659-d5r5j
49aee3bbddc51       97e04611ad434                                                                                            35 minutes ago      Exited              coredns                   0                   c3c394540e069       coredns-5dd5756b68-5czpx
627c6923118ff       a5dd5cdd6d3ef                                                                                            35 minutes ago      Exited              kube-proxy                0                   c3274a7dfc917       kube-proxy-79dxz
95c8d5174e5d8       537e9a59ee2fd                                                                                            35 minutes ago      Exited              kube-apiserver            0                   745dad8d6186d       kube-apiserver-minikube
1ee18d8071a9d       8276439b4f237                                                                                            35 minutes ago      Exited              kube-controller-manager   0                   5bb469500b6f5       kube-controller-manager-minikube
b23be9d2fd5fc       42a4e73724daa                                                                                            35 minutes ago      Exited              kube-scheduler            0                   065509fb18d74       kube-scheduler-minikube
ed734d64ec433       9cdd6470f48c8                                                                                            35 minutes ago      Exited              etcd                      0                   c9bee2941b5c9       etcd-minikube

* 
* ==> coredns [49aee3bbddc5] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/arm64, go1.20, 055b2c3
[INFO] 127.0.0.1:56091 - 34859 "HINFO IN 6300563902218674931.4307021880332149904. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.0911835s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> coredns [df0f905c80f3] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/arm64, go1.20, 055b2c3
[INFO] 127.0.0.1:56216 - 34800 "HINFO IN 1167012744661766014.147494657463124761. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.071796834s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_01_22T17_39_07_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 22 Jan 2024 16:39:04 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 22 Jan 2024 17:14:56 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 22 Jan 2024 17:07:34 +0000   Mon, 22 Jan 2024 16:39:04 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 22 Jan 2024 17:07:34 +0000   Mon, 22 Jan 2024 16:39:04 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 22 Jan 2024 17:07:34 +0000   Mon, 22 Jan 2024 16:39:04 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 22 Jan 2024 17:07:34 +0000   Mon, 22 Jan 2024 16:39:04 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                10
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8039720Ki
  pods:               110
Allocatable:
  cpu:                10
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8039720Ki
  pods:               110
System Info:
  Machine ID:                 f2d627f25b5d4b9c894fa2477f5b330c
  System UUID:                f2d627f25b5d4b9c894fa2477f5b330c
  Boot ID:                    246396e5-b712-44f4-a162-81764589ab55
  Kernel Version:             6.4.16-linuxkit
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                      CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                      ------------  ----------  ---------------  -------------  ---
  default                     auth-deployment-97d67ff76-64xkq           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23m
  default                     mysql-user-deployment-6dc8457659-d5r5j    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         34m
  kube-system                 coredns-5dd5756b68-5czpx                  100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     35m
  kube-system                 etcd-minikube                             100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         35m
  kube-system                 kube-apiserver-minikube                   250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         35m
  kube-system                 kube-controller-manager-minikube          200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         35m
  kube-system                 kube-proxy-79dxz                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         35m
  kube-system                 kube-scheduler-minikube                   100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         35m
  kube-system                 storage-provisioner                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         35m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (7%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-32Mi     0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-64Ki     0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 35m                kube-proxy       
  Normal  Starting                 26m                kube-proxy       
  Normal  NodeHasSufficientPID     35m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  35m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  35m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    35m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  Starting                 35m                kubelet          Starting kubelet.
  Normal  RegisteredNode           35m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 26m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  26m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  26m (x8 over 26m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    26m (x8 over 26m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     26m (x7 over 26m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           26m                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Jan22 16:33] cacheinfo: Unable to detect cache hierarchy for CPU 0
[  +0.156942] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.942950] 3[323]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +0.370427] FAT-fs (loop0): utf8 is not a recommended IO charset for FAT filesystems, filesystem will be case sensitive!
[  +0.000294] FAT-fs (loop0): utf8 is not a recommended IO charset for FAT filesystems, filesystem will be case sensitive!
[  +0.011355] grpcfuse: loading out-of-tree module taints kernel.

* 
* ==> etcd [18fb1813b461] <==
* {"level":"warn","ts":"2024-01-22T16:48:05.624453Z","caller":"embed/config.go:673","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-01-22T16:48:05.624501Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-01-22T16:48:05.624553Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2024-01-22T16:48:05.624567Z","caller":"embed/config.go:673","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-01-22T16:48:05.624574Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-01-22T16:48:05.624595Z","caller":"embed/etcd.go:495","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-01-22T16:48:05.624873Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-01-22T16:48:05.624941Z","caller":"embed/etcd.go:309","msg":"starting an etcd server","etcd-version":"3.5.9","git-sha":"bdbbde998","go-version":"go1.19.9","go-os":"linux","go-arch":"arm64","max-cpu-set":10,"max-cpu-available":10,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-01-22T16:48:05.651152Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"26.079792ms"}
{"level":"info","ts":"2024-01-22T16:48:05.65477Z","caller":"etcdserver/server.go:530","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2024-01-22T16:48:05.658683Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":941}
{"level":"info","ts":"2024-01-22T16:48:05.658777Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-01-22T16:48:05.658816Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 2"}
{"level":"info","ts":"2024-01-22T16:48:05.658821Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 2, commit: 941, applied: 0, lastindex: 941, lastterm: 2]"}
{"level":"warn","ts":"2024-01-22T16:48:05.65946Z","caller":"auth/store.go:1238","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-01-22T16:48:05.661376Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":831}
{"level":"info","ts":"2024-01-22T16:48:05.661908Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-01-22T16:48:05.662516Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-01-22T16:48:05.66272Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-01-22T16:48:05.662745Z","caller":"etcdserver/server.go:854","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.9","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-01-22T16:48:05.662909Z","caller":"etcdserver/server.go:754","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2024-01-22T16:48:05.663002Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-22T16:48:05.663052Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-22T16:48:05.663059Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-22T16:48:05.663323Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-01-22T16:48:05.663366Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-01-22T16:48:05.663415Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-22T16:48:05.663428Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-22T16:48:05.663768Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-01-22T16:48:05.663864Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-01-22T16:48:05.663872Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-01-22T16:48:05.663983Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-01-22T16:48:05.66399Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-01-22T16:48:06.659496Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 2"}
{"level":"info","ts":"2024-01-22T16:48:06.659674Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 2"}
{"level":"info","ts":"2024-01-22T16:48:06.659806Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-01-22T16:48:06.659864Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2024-01-22T16:48:06.659884Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-01-22T16:48:06.65991Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2024-01-22T16:48:06.659945Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-01-22T16:48:06.662996Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-01-22T16:48:06.663056Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-01-22T16:48:06.663004Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-01-22T16:48:06.665665Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-01-22T16:48:06.665721Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-01-22T16:48:06.666016Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-01-22T16:48:06.66699Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-01-22T16:58:06.693409Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1252}
{"level":"info","ts":"2024-01-22T16:58:06.731358Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":1252,"took":"37.512ms","hash":3579502639}
{"level":"info","ts":"2024-01-22T16:58:06.731392Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3579502639,"revision":1252,"compact-revision":-1}
{"level":"info","ts":"2024-01-22T17:03:06.661478Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1491}
{"level":"info","ts":"2024-01-22T17:03:06.665276Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":1491,"took":"2.939708ms","hash":1111852725}
{"level":"info","ts":"2024-01-22T17:03:06.665349Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1111852725,"revision":1491,"compact-revision":1252}
{"level":"info","ts":"2024-01-22T17:08:06.665058Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1731}
{"level":"info","ts":"2024-01-22T17:08:06.668182Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":1731,"took":"2.603ms","hash":328607294}
{"level":"info","ts":"2024-01-22T17:08:06.668237Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":328607294,"revision":1731,"compact-revision":1491}

* 
* ==> etcd [ed734d64ec43] <==
* {"level":"warn","ts":"2024-01-22T16:39:03.409187Z","caller":"embed/config.go:673","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-01-22T16:39:03.409285Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2024-01-22T16:39:03.409345Z","caller":"embed/config.go:673","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-01-22T16:39:03.409354Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-01-22T16:39:03.409391Z","caller":"embed/etcd.go:495","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-01-22T16:39:03.409696Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-01-22T16:39:03.409789Z","caller":"embed/etcd.go:309","msg":"starting an etcd server","etcd-version":"3.5.9","git-sha":"bdbbde998","go-version":"go1.19.9","go-os":"linux","go-arch":"arm64","max-cpu-set":10,"max-cpu-available":10,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-01-22T16:39:03.411089Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"1.075542ms"}
{"level":"info","ts":"2024-01-22T16:39:03.413266Z","caller":"etcdserver/raft.go:495","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2024-01-22T16:39:03.413346Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-01-22T16:39:03.413375Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2024-01-22T16:39:03.413383Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2024-01-22T16:39:03.41341Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2024-01-22T16:39:03.413434Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2024-01-22T16:39:03.415107Z","caller":"auth/store.go:1238","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-01-22T16:39:03.416486Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2024-01-22T16:39:03.417241Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-01-22T16:39:03.417852Z","caller":"etcdserver/server.go:854","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.9","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-01-22T16:39:03.417912Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-01-22T16:39:03.418064Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-22T16:39:03.418115Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-22T16:39:03.418126Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-22T16:39:03.418969Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-01-22T16:39:03.419047Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-01-22T16:39:03.419243Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-01-22T16:39:03.419367Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-01-22T16:39:03.419387Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-01-22T16:39:03.419403Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-01-22T16:39:03.419608Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-01-22T16:39:03.514013Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2024-01-22T16:39:03.514037Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2024-01-22T16:39:03.514044Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2024-01-22T16:39:03.51405Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2024-01-22T16:39:03.514053Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-01-22T16:39:03.514067Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2024-01-22T16:39:03.514073Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-01-22T16:39:03.514366Z","caller":"etcdserver/server.go:2571","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-22T16:39:03.514741Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-01-22T16:39:03.514761Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-01-22T16:39:03.514843Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-01-22T16:39:03.514911Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-22T16:39:03.514977Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-22T16:39:03.514992Z","caller":"etcdserver/server.go:2595","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-22T16:39:03.515371Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-01-22T16:39:03.516114Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-01-22T16:39:03.516202Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-01-22T16:39:03.516409Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-01-22T16:47:15.006835Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-01-22T16:47:15.006959Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-01-22T16:47:15.007057Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-01-22T16:47:15.007125Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
WARNING: 2024/01/22 16:47:15 [core] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"warn","ts":"2024-01-22T16:47:15.016345Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-01-22T16:47:15.016377Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-01-22T16:47:15.016417Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-01-22T16:47:15.020598Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-01-22T16:47:15.020679Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-01-22T16:47:15.020686Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> kernel <==
*  17:15:01 up 41 min,  0 users,  load average: 0.44, 0.55, 0.52
Linux minikube 6.4.16-linuxkit #1 SMP PREEMPT Tue Oct 10 20:38:06 UTC 2023 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [6046d13cc483] <==
* W0122 16:48:06.961262       1 genericapiserver.go:744] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0122 16:48:06.971440       1 handler.go:232] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0122 16:48:06.971455       1 genericapiserver.go:744] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0122 16:48:07.287253       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0122 16:48:07.287363       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0122 16:48:07.287384       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0122 16:48:07.287771       1 secure_serving.go:213] Serving securely on [::]:8443
I0122 16:48:07.287829       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0122 16:48:07.287886       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0122 16:48:07.287909       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0122 16:48:07.287922       1 aggregator.go:164] waiting for initial CRD sync...
I0122 16:48:07.288002       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0122 16:48:07.288013       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0122 16:48:07.288023       1 controller.go:78] Starting OpenAPI AggregationController
I0122 16:48:07.288007       1 apf_controller.go:372] Starting API Priority and Fairness config controller
I0122 16:48:07.288058       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0122 16:48:07.288068       1 controller.go:116] Starting legacy_token_tracking_controller
I0122 16:48:07.288073       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I0122 16:48:07.288269       1 available_controller.go:423] Starting AvailableConditionController
I0122 16:48:07.288330       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0122 16:48:07.288384       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0122 16:48:07.288404       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0122 16:48:07.288452       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0122 16:48:07.288483       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0122 16:48:07.288491       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0122 16:48:07.288499       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0122 16:48:07.288535       1 handler_discovery.go:412] Starting ResourceDiscoveryManager
I0122 16:48:07.288583       1 controller.go:134] Starting OpenAPI controller
I0122 16:48:07.288607       1 controller.go:85] Starting OpenAPI V3 controller
I0122 16:48:07.288616       1 naming_controller.go:291] Starting NamingConditionController
I0122 16:48:07.288634       1 establishing_controller.go:76] Starting EstablishingController
I0122 16:48:07.288643       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0122 16:48:07.288648       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0122 16:48:07.288658       1 crd_finalizer.go:266] Starting CRDFinalizer
I0122 16:48:07.288672       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0122 16:48:07.288709       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0122 16:48:07.288279       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0122 16:48:07.352137       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0122 16:48:07.354064       1 shared_informer.go:318] Caches are synced for node_authorizer
I0122 16:48:07.388540       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0122 16:48:07.388585       1 aggregator.go:166] initial CRD sync complete...
I0122 16:48:07.388590       1 autoregister_controller.go:141] Starting autoregister controller
I0122 16:48:07.388594       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0122 16:48:07.388597       1 cache.go:39] Caches are synced for autoregister controller
I0122 16:48:07.388602       1 shared_informer.go:318] Caches are synced for configmaps
I0122 16:48:07.388620       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0122 16:48:07.388659       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0122 16:48:07.388765       1 apf_controller.go:377] Running API Priority and Fairness config worker
I0122 16:48:07.388774       1 apf_controller.go:380] Running API Priority and Fairness periodic rebalancing process
I0122 16:48:07.388825       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0122 16:48:08.292065       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
W0122 16:48:08.551180       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0122 16:48:08.552049       1 controller.go:624] quota admission added evaluator for: endpoints
I0122 16:48:08.946035       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0122 16:48:08.949935       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0122 16:48:08.961608       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0122 16:48:08.977137       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0122 16:48:08.980446       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0122 16:48:20.202075       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0122 16:51:36.561183       1 controller.go:624] quota admission added evaluator for: replicasets.apps

* 
* ==> kube-apiserver [95c8d5174e5d] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0122 16:47:16.013329       1 logging.go:59] [core] [Channel #28 SubChannel #29] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0122 16:47:16.014573       1 logging.go:59] [core] [Channel #34 SubChannel #35] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0122 16:47:16.014613       1 logging.go:59] [core] [Channel #52 SubChannel #53] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0122 16:47:16.014770       1 logging.go:59] [core] [Channel #85 SubChannel #86] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0122 16:47:16.014786       1 logging.go:59] [core] [Channel #97 SubChannel #98] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0122 16:47:16.014764       1 logging.go:59] [core] [Channel #115 SubChannel #116] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0122 16:47:16.014882       1 logging.go:59] [core] [Channel #166 SubChannel #167] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-controller-manager [1ee18d8071a9] <==
* I0122 16:39:19.308810       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0122 16:39:19.308800       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0122 16:39:19.308818       1 shared_informer.go:318] Caches are synced for endpoint
I0122 16:39:19.308875       1 shared_informer.go:318] Caches are synced for ephemeral
I0122 16:39:19.308771       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0122 16:39:19.309121       1 shared_informer.go:318] Caches are synced for ReplicationController
I0122 16:39:19.310037       1 shared_informer.go:318] Caches are synced for PVC protection
I0122 16:39:19.310394       1 shared_informer.go:318] Caches are synced for service account
I0122 16:39:19.310560       1 shared_informer.go:318] Caches are synced for PV protection
I0122 16:39:19.314630       1 shared_informer.go:318] Caches are synced for disruption
I0122 16:39:19.317113       1 shared_informer.go:318] Caches are synced for deployment
I0122 16:39:19.324487       1 shared_informer.go:318] Caches are synced for HPA
I0122 16:39:19.400030       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0122 16:39:19.414786       1 shared_informer.go:318] Caches are synced for crt configmap
I0122 16:39:19.429978       1 shared_informer.go:318] Caches are synced for resource quota
I0122 16:39:19.430040       1 shared_informer.go:318] Caches are synced for resource quota
I0122 16:39:19.464560       1 shared_informer.go:318] Caches are synced for taint
I0122 16:39:19.464778       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0122 16:39:19.464888       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0122 16:39:19.464921       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0122 16:39:19.464952       1 taint_manager.go:211] "Sending events to api server"
I0122 16:39:19.464979       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0122 16:39:19.465150       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0122 16:39:19.471437       1 shared_informer.go:318] Caches are synced for persistent volume
I0122 16:39:19.837083       1 shared_informer.go:318] Caches are synced for garbage collector
I0122 16:39:19.914508       1 shared_informer.go:318] Caches are synced for garbage collector
I0122 16:39:19.914575       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0122 16:39:20.078850       1 event.go:307] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-79dxz"
I0122 16:39:20.126996       1 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-5dd5756b68 to 1"
I0122 16:39:20.326979       1 event.go:307] "Event occurred" object="kube-system/coredns-5dd5756b68" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-5dd5756b68-5czpx"
I0122 16:39:20.330018       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="203.151292ms"
I0122 16:39:20.335485       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="5.445167ms"
I0122 16:39:20.335524       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="22.25¬µs"
I0122 16:39:21.233290       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="2.900459ms"
I0122 16:39:21.233340       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="30.084¬µs"
I0122 16:40:27.460118       1 event.go:307] "Event occurred" object="default/mysql-user-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set mysql-user-deployment-6dc8457659 to 1"
I0122 16:40:27.467063       1 event.go:307] "Event occurred" object="default/mysql-user-deployment-6dc8457659" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mysql-user-deployment-6dc8457659-d5r5j"
I0122 16:40:27.472388       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mysql-user-deployment-6dc8457659" duration="12.665458ms"
I0122 16:40:27.482223       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mysql-user-deployment-6dc8457659" duration="9.770917ms"
I0122 16:40:27.511224       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mysql-user-deployment-6dc8457659" duration="28.934ms"
I0122 16:40:27.511331       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mysql-user-deployment-6dc8457659" duration="30.917¬µs"
I0122 16:40:27.511655       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mysql-user-deployment-6dc8457659" duration="44.542¬µs"
I0122 16:40:38.994674       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mysql-user-deployment-6dc8457659" duration="3.608875ms"
I0122 16:40:38.994806       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mysql-user-deployment-6dc8457659" duration="71.25¬µs"
I0122 16:43:27.273633       1 event.go:307] "Event occurred" object="default/auth-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set auth-deployment-54f94f4bb5 to 1"
I0122 16:43:27.281477       1 event.go:307] "Event occurred" object="default/auth-deployment-54f94f4bb5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: auth-deployment-54f94f4bb5-crrkg"
I0122 16:43:27.286755       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="13.912334ms"
I0122 16:43:27.291019       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="4.215292ms"
I0122 16:43:27.291108       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="61.625¬µs"
I0122 16:43:27.298009       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="39.917¬µs"
I0122 16:43:28.608569       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="47.583¬µs"
I0122 16:43:42.114867       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="486.709¬µs"
I0122 16:43:57.105178       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="166.417¬µs"
I0122 16:44:09.100666       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="54.875¬µs"
I0122 16:44:20.101318       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="78.417¬µs"
I0122 16:44:32.102301       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="52.125¬µs"
I0122 16:45:11.106225       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="332.542¬µs"
I0122 16:45:22.103042       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="101¬µs"
I0122 16:46:42.104533       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="331.084¬µs"
I0122 16:46:54.109828       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="163.334¬µs"

* 
* ==> kube-controller-manager [39a65e87451b] <==
* I0122 16:48:20.224342       1 shared_informer.go:318] Caches are synced for ReplicationController
I0122 16:48:20.225590       1 shared_informer.go:318] Caches are synced for deployment
I0122 16:48:20.225832       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0122 16:48:20.225927       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="41¬µs"
I0122 16:48:20.225977       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="26.584¬µs"
I0122 16:48:20.225937       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mysql-user-deployment-6dc8457659" duration="29.458¬µs"
I0122 16:48:20.227232       1 shared_informer.go:318] Caches are synced for endpoint
I0122 16:48:20.262244       1 shared_informer.go:318] Caches are synced for daemon sets
I0122 16:48:20.264785       1 shared_informer.go:318] Caches are synced for PVC protection
I0122 16:48:20.269571       1 shared_informer.go:318] Caches are synced for GC
I0122 16:48:20.271367       1 shared_informer.go:318] Caches are synced for disruption
I0122 16:48:20.273646       1 shared_informer.go:318] Caches are synced for HPA
I0122 16:48:20.302691       1 shared_informer.go:318] Caches are synced for resource quota
I0122 16:48:20.308321       1 shared_informer.go:318] Caches are synced for TTL after finished
I0122 16:48:20.324003       1 shared_informer.go:318] Caches are synced for job
I0122 16:48:20.363878       1 shared_informer.go:318] Caches are synced for cronjob
I0122 16:48:20.377804       1 shared_informer.go:318] Caches are synced for resource quota
I0122 16:48:20.695201       1 shared_informer.go:318] Caches are synced for garbage collector
I0122 16:48:20.723839       1 shared_informer.go:318] Caches are synced for garbage collector
I0122 16:48:20.723913       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0122 16:48:20.980483       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="40.875¬µs"
I0122 16:48:31.983982       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="63.583¬µs"
I0122 16:48:36.731084       1 event.go:307] "Event occurred" object="default/auth-deployment-54f94f4bb5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: auth-deployment-54f94f4bb5-r6fmq"
I0122 16:48:36.734182       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="21.212292ms"
I0122 16:48:36.736601       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="2.395709ms"
I0122 16:48:36.736650       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="20.834¬µs"
I0122 16:48:36.737997       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="21.625¬µs"
I0122 16:48:36.802649       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="33.833¬µs"
I0122 16:48:37.424165       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="65.583¬µs"
I0122 16:48:37.428984       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="34.875¬µs"
I0122 16:48:37.431597       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="26.209¬µs"
I0122 16:48:37.432642       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="13¬µs"
I0122 16:48:37.436719       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="21.542¬µs"
I0122 16:48:49.994612       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="121.125¬µs"
I0122 16:49:04.993908       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="68.25¬µs"
I0122 16:49:16.997764       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="487.5¬µs"
I0122 16:49:31.992608       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="193.792¬µs"
I0122 16:49:44.989635       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="231.708¬µs"
I0122 16:50:11.995770       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="1.151917ms"
I0122 16:50:26.999660       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="176.541¬µs"
I0122 16:51:36.566469       1 event.go:307] "Event occurred" object="default/auth-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set auth-deployment-97d67ff76 to 1"
I0122 16:51:36.570626       1 event.go:307] "Event occurred" object="default/auth-deployment-97d67ff76" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: auth-deployment-97d67ff76-64xkq"
I0122 16:51:36.574493       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-97d67ff76" duration="8.436ms"
I0122 16:51:36.578460       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-97d67ff76" duration="3.922209ms"
I0122 16:51:36.578514       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-97d67ff76" duration="31.917¬µs"
I0122 16:51:36.584765       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-97d67ff76" duration="244.959¬µs"
I0122 16:51:45.980002       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="162.958¬µs"
I0122 16:51:46.187551       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-97d67ff76" duration="3.574ms"
I0122 16:51:46.187633       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-97d67ff76" duration="27.792¬µs"
I0122 16:51:46.190181       1 event.go:307] "Event occurred" object="default/auth-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set auth-deployment-54f94f4bb5 to 0 from 1"
I0122 16:51:46.247170       1 event.go:307] "Event occurred" object="default/auth-deployment-54f94f4bb5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: auth-deployment-54f94f4bb5-r6fmq"
I0122 16:51:46.251193       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="61.2755ms"
I0122 16:51:46.254315       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="3.088125ms"
I0122 16:51:46.254394       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="56.459¬µs"
I0122 16:51:46.255224       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="15.125¬µs"
I0122 16:51:46.382668       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="31.458¬µs"
I0122 16:51:47.254082       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="34.416¬µs"
I0122 16:51:47.258412       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="18.291¬µs"
I0122 16:51:47.260251       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="18.375¬µs"
I0122 16:51:47.261559       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/auth-deployment-54f94f4bb5" duration="32.208¬µs"

* 
* ==> kube-proxy [627c6923118f] <==
* I0122 16:39:20.547563       1 server_others.go:69] "Using iptables proxy"
I0122 16:39:20.553010       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0122 16:39:20.563854       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0122 16:39:20.564837       1 server_others.go:152] "Using iptables Proxier"
I0122 16:39:20.564853       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0122 16:39:20.564856       1 server_others.go:438] "Defaulting to no-op detect-local"
I0122 16:39:20.564877       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0122 16:39:20.565090       1 server.go:846] "Version info" version="v1.28.3"
I0122 16:39:20.565101       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0122 16:39:20.565558       1 config.go:188] "Starting service config controller"
I0122 16:39:20.565601       1 shared_informer.go:311] Waiting for caches to sync for service config
I0122 16:39:20.565623       1 config.go:315] "Starting node config controller"
I0122 16:39:20.565642       1 shared_informer.go:311] Waiting for caches to sync for node config
I0122 16:39:20.565671       1 config.go:97] "Starting endpoint slice config controller"
I0122 16:39:20.565673       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0122 16:39:20.666293       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0122 16:39:20.666324       1 shared_informer.go:318] Caches are synced for node config
I0122 16:39:20.666324       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-proxy [a2d0ee08b8f7] <==
* I0122 16:48:08.673598       1 server_others.go:69] "Using iptables proxy"
I0122 16:48:08.679089       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0122 16:48:08.691399       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0122 16:48:08.692487       1 server_others.go:152] "Using iptables Proxier"
I0122 16:48:08.692517       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0122 16:48:08.692521       1 server_others.go:438] "Defaulting to no-op detect-local"
I0122 16:48:08.692559       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0122 16:48:08.692764       1 server.go:846] "Version info" version="v1.28.3"
I0122 16:48:08.692777       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0122 16:48:08.693420       1 config.go:97] "Starting endpoint slice config controller"
I0122 16:48:08.693440       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0122 16:48:08.693489       1 config.go:188] "Starting service config controller"
I0122 16:48:08.693495       1 shared_informer.go:311] Waiting for caches to sync for service config
I0122 16:48:08.693506       1 config.go:315] "Starting node config controller"
I0122 16:48:08.693511       1 shared_informer.go:311] Waiting for caches to sync for node config
I0122 16:48:08.793573       1 shared_informer.go:318] Caches are synced for service config
I0122 16:48:08.793605       1 shared_informer.go:318] Caches are synced for node config
I0122 16:48:08.793627       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-scheduler [43ba01a57049] <==
* I0122 16:48:05.803633       1 serving.go:348] Generated self-signed cert in-memory
W0122 16:48:07.349565       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0122 16:48:07.349592       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0122 16:48:07.349599       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0122 16:48:07.349603       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0122 16:48:07.357875       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0122 16:48:07.357889       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0122 16:48:07.358710       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0122 16:48:07.358737       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0122 16:48:07.359023       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0122 16:48:07.359051       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0122 16:48:07.459146       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [b23be9d2fd5f] <==
* I0122 16:39:03.744175       1 serving.go:348] Generated self-signed cert in-memory
W0122 16:39:04.442246       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0122 16:39:04.442282       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0122 16:39:04.442295       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0122 16:39:04.442301       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0122 16:39:04.449902       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0122 16:39:04.449936       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0122 16:39:04.450721       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0122 16:39:04.450755       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0122 16:39:04.451053       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0122 16:39:04.451075       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W0122 16:39:04.509098       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0122 16:39:04.509133       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0122 16:39:04.509151       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0122 16:39:04.509171       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0122 16:39:04.509170       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0122 16:39:04.509179       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0122 16:39:04.509187       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0122 16:39:04.509173       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0122 16:39:04.509219       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0122 16:39:04.509223       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0122 16:39:04.509230       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0122 16:39:04.509225       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0122 16:39:04.509237       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0122 16:39:04.509253       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0122 16:39:04.509281       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0122 16:39:04.509298       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0122 16:39:04.509311       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0122 16:39:04.509319       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0122 16:39:04.509328       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0122 16:39:04.509336       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0122 16:39:04.509428       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0122 16:39:04.509442       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0122 16:39:04.509470       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0122 16:39:04.509475       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0122 16:39:04.509697       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0122 16:39:04.509727       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0122 16:39:04.509753       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0122 16:39:04.509759       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0122 16:39:04.509762       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0122 16:39:04.509768       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0122 16:39:05.327928       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0122 16:39:05.328061       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0122 16:39:05.415815       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0122 16:39:05.415930       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0122 16:39:05.417802       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0122 16:39:05.417840       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0122 16:39:05.475919       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0122 16:39:05.475987       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0122 16:39:05.551785       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0122 16:39:05.551804       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
I0122 16:39:05.851494       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0122 16:47:14.984197       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0122 16:47:14.984303       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0122 16:47:14.984445       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0122 16:47:14.984585       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* Jan 22 16:48:10 minikube kubelet[1473]: I0122 16:48:10.147462    1473 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Jan 22 16:48:20 minikube kubelet[1473]: E0122 16:48:20.980920    1473 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused" image="localhost:5000/spring-multi-repo-app-auth:latest"
Jan 22 16:48:20 minikube kubelet[1473]: E0122 16:48:20.980945    1473 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused" image="localhost:5000/spring-multi-repo-app-auth:latest"
Jan 22 16:48:20 minikube kubelet[1473]: E0122 16:48:20.981505    1473 kuberuntime_manager.go:1256] container &Container{Name:auth,Image:localhost:5000/spring-multi-repo-app-auth,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jvsjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod auth-deployment-54f94f4bb5-crrkg_default(1c983def-fdca-45d1-beea-7286afbcc1a9): ErrImagePull: Error response from daemon: Get "http://localhost:5000/v2/": dial tcp 127.0.0.1:5000: connect: connection refused
Jan 22 16:48:20 minikube kubelet[1473]: E0122 16:48:20.981536    1473 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth\" with ErrImagePull: \"Error response from daemon: Get \\\"http://localhost:5000/v2/\\\": dial tcp 127.0.0.1:5000: connect: connection refused\"" pod="default/auth-deployment-54f94f4bb5-crrkg" podUID="1c983def-fdca-45d1-beea-7286afbcc1a9"
Jan 22 16:48:22 minikube kubelet[1473]: I0122 16:48:22.974946    1473 scope.go:117] "RemoveContainer" containerID="8f67926b73171e55c994425bbedcfe642572db79e60908276917fa04fe80383d"
Jan 22 16:48:31 minikube kubelet[1473]: E0122 16:48:31.977337    1473 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth\" with ImagePullBackOff: \"Back-off pulling image \\\"localhost:5000/spring-multi-repo-app-auth\\\"\"" pod="default/auth-deployment-54f94f4bb5-crrkg" podUID="1c983def-fdca-45d1-beea-7286afbcc1a9"
Jan 22 16:48:36 minikube kubelet[1473]: I0122 16:48:36.733761    1473 topology_manager.go:215] "Topology Admit Handler" podUID="eb535cfe-01c3-4d17-b812-2e2aa9c42437" podNamespace="default" podName="auth-deployment-54f94f4bb5-r6fmq"
Jan 22 16:48:36 minikube kubelet[1473]: I0122 16:48:36.849903    1473 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-jvsjv\" (UniqueName: \"kubernetes.io/projected/1c983def-fdca-45d1-beea-7286afbcc1a9-kube-api-access-jvsjv\") pod \"1c983def-fdca-45d1-beea-7286afbcc1a9\" (UID: \"1c983def-fdca-45d1-beea-7286afbcc1a9\") "
Jan 22 16:48:36 minikube kubelet[1473]: I0122 16:48:36.850017    1473 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qgn6f\" (UniqueName: \"kubernetes.io/projected/eb535cfe-01c3-4d17-b812-2e2aa9c42437-kube-api-access-qgn6f\") pod \"auth-deployment-54f94f4bb5-r6fmq\" (UID: \"eb535cfe-01c3-4d17-b812-2e2aa9c42437\") " pod="default/auth-deployment-54f94f4bb5-r6fmq"
Jan 22 16:48:36 minikube kubelet[1473]: I0122 16:48:36.852781    1473 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/1c983def-fdca-45d1-beea-7286afbcc1a9-kube-api-access-jvsjv" (OuterVolumeSpecName: "kube-api-access-jvsjv") pod "1c983def-fdca-45d1-beea-7286afbcc1a9" (UID: "1c983def-fdca-45d1-beea-7286afbcc1a9"). InnerVolumeSpecName "kube-api-access-jvsjv". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jan 22 16:48:36 minikube kubelet[1473]: I0122 16:48:36.950700    1473 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-jvsjv\" (UniqueName: \"kubernetes.io/projected/1c983def-fdca-45d1-beea-7286afbcc1a9-kube-api-access-jvsjv\") on node \"minikube\" DevicePath \"\""
Jan 22 16:48:37 minikube kubelet[1473]: E0122 16:48:37.146256    1473 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused" image="localhost:5000/spring-multi-repo-app-auth:latest"
Jan 22 16:48:37 minikube kubelet[1473]: E0122 16:48:37.146289    1473 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused" image="localhost:5000/spring-multi-repo-app-auth:latest"
Jan 22 16:48:37 minikube kubelet[1473]: E0122 16:48:37.146333    1473 kuberuntime_manager.go:1256] container &Container{Name:auth,Image:localhost:5000/spring-multi-repo-app-auth,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qgn6f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod auth-deployment-54f94f4bb5-r6fmq_default(eb535cfe-01c3-4d17-b812-2e2aa9c42437): ErrImagePull: Error response from daemon: Get "http://localhost:5000/v2/": dial tcp 127.0.0.1:5000: connect: connection refused
Jan 22 16:48:37 minikube kubelet[1473]: E0122 16:48:37.146356    1473 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth\" with ErrImagePull: \"Error response from daemon: Get \\\"http://localhost:5000/v2/\\\": dial tcp 127.0.0.1:5000: connect: connection refused\"" pod="default/auth-deployment-54f94f4bb5-r6fmq" podUID="eb535cfe-01c3-4d17-b812-2e2aa9c42437"
Jan 22 16:48:37 minikube kubelet[1473]: E0122 16:48:37.423510    1473 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth\" with ImagePullBackOff: \"Back-off pulling image \\\"localhost:5000/spring-multi-repo-app-auth\\\"\"" pod="default/auth-deployment-54f94f4bb5-r6fmq" podUID="eb535cfe-01c3-4d17-b812-2e2aa9c42437"
Jan 22 16:48:38 minikube kubelet[1473]: I0122 16:48:38.986551    1473 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="1c983def-fdca-45d1-beea-7286afbcc1a9" path="/var/lib/kubelet/pods/1c983def-fdca-45d1-beea-7286afbcc1a9/volumes"
Jan 22 16:48:49 minikube kubelet[1473]: E0122 16:48:49.993213    1473 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused" image="localhost:5000/spring-multi-repo-app-auth:latest"
Jan 22 16:48:49 minikube kubelet[1473]: E0122 16:48:49.993319    1473 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused" image="localhost:5000/spring-multi-repo-app-auth:latest"
Jan 22 16:48:49 minikube kubelet[1473]: E0122 16:48:49.993469    1473 kuberuntime_manager.go:1256] container &Container{Name:auth,Image:localhost:5000/spring-multi-repo-app-auth,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qgn6f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod auth-deployment-54f94f4bb5-r6fmq_default(eb535cfe-01c3-4d17-b812-2e2aa9c42437): ErrImagePull: Error response from daemon: Get "http://localhost:5000/v2/": dial tcp 127.0.0.1:5000: connect: connection refused
Jan 22 16:48:49 minikube kubelet[1473]: E0122 16:48:49.993541    1473 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth\" with ErrImagePull: \"Error response from daemon: Get \\\"http://localhost:5000/v2/\\\": dial tcp 127.0.0.1:5000: connect: connection refused\"" pod="default/auth-deployment-54f94f4bb5-r6fmq" podUID="eb535cfe-01c3-4d17-b812-2e2aa9c42437"
Jan 22 16:49:04 minikube kubelet[1473]: E0122 16:49:04.983021    1473 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth\" with ImagePullBackOff: \"Back-off pulling image \\\"localhost:5000/spring-multi-repo-app-auth\\\"\"" pod="default/auth-deployment-54f94f4bb5-r6fmq" podUID="eb535cfe-01c3-4d17-b812-2e2aa9c42437"
Jan 22 16:49:17 minikube kubelet[1473]: E0122 16:49:17.002941    1473 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused" image="localhost:5000/spring-multi-repo-app-auth:latest"
Jan 22 16:49:17 minikube kubelet[1473]: E0122 16:49:17.003038    1473 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused" image="localhost:5000/spring-multi-repo-app-auth:latest"
Jan 22 16:49:17 minikube kubelet[1473]: E0122 16:49:17.003261    1473 kuberuntime_manager.go:1256] container &Container{Name:auth,Image:localhost:5000/spring-multi-repo-app-auth,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qgn6f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod auth-deployment-54f94f4bb5-r6fmq_default(eb535cfe-01c3-4d17-b812-2e2aa9c42437): ErrImagePull: Error response from daemon: Get "http://localhost:5000/v2/": dial tcp 127.0.0.1:5000: connect: connection refused
Jan 22 16:49:17 minikube kubelet[1473]: E0122 16:49:17.003344    1473 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth\" with ErrImagePull: \"Error response from daemon: Get \\\"http://localhost:5000/v2/\\\": dial tcp 127.0.0.1:5000: connect: connection refused\"" pod="default/auth-deployment-54f94f4bb5-r6fmq" podUID="eb535cfe-01c3-4d17-b812-2e2aa9c42437"
Jan 22 16:49:31 minikube kubelet[1473]: E0122 16:49:31.979927    1473 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth\" with ImagePullBackOff: \"Back-off pulling image \\\"localhost:5000/spring-multi-repo-app-auth\\\"\"" pod="default/auth-deployment-54f94f4bb5-r6fmq" podUID="eb535cfe-01c3-4d17-b812-2e2aa9c42437"
Jan 22 16:49:44 minikube kubelet[1473]: E0122 16:49:44.978849    1473 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth\" with ImagePullBackOff: \"Back-off pulling image \\\"localhost:5000/spring-multi-repo-app-auth\\\"\"" pod="default/auth-deployment-54f94f4bb5-r6fmq" podUID="eb535cfe-01c3-4d17-b812-2e2aa9c42437"
Jan 22 16:49:59 minikube kubelet[1473]: E0122 16:49:59.989815    1473 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused" image="localhost:5000/spring-multi-repo-app-auth:latest"
Jan 22 16:49:59 minikube kubelet[1473]: E0122 16:49:59.989975    1473 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused" image="localhost:5000/spring-multi-repo-app-auth:latest"
Jan 22 16:49:59 minikube kubelet[1473]: E0122 16:49:59.990279    1473 kuberuntime_manager.go:1256] container &Container{Name:auth,Image:localhost:5000/spring-multi-repo-app-auth,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qgn6f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod auth-deployment-54f94f4bb5-r6fmq_default(eb535cfe-01c3-4d17-b812-2e2aa9c42437): ErrImagePull: Error response from daemon: Get "http://localhost:5000/v2/": dial tcp 127.0.0.1:5000: connect: connection refused
Jan 22 16:49:59 minikube kubelet[1473]: E0122 16:49:59.990375    1473 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth\" with ErrImagePull: \"Error response from daemon: Get \\\"http://localhost:5000/v2/\\\": dial tcp 127.0.0.1:5000: connect: connection refused\"" pod="default/auth-deployment-54f94f4bb5-r6fmq" podUID="eb535cfe-01c3-4d17-b812-2e2aa9c42437"
Jan 22 16:50:11 minikube kubelet[1473]: E0122 16:50:11.979549    1473 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth\" with ImagePullBackOff: \"Back-off pulling image \\\"localhost:5000/spring-multi-repo-app-auth\\\"\"" pod="default/auth-deployment-54f94f4bb5-r6fmq" podUID="eb535cfe-01c3-4d17-b812-2e2aa9c42437"
Jan 22 16:50:26 minikube kubelet[1473]: E0122 16:50:26.987232    1473 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth\" with ImagePullBackOff: \"Back-off pulling image \\\"localhost:5000/spring-multi-repo-app-auth\\\"\"" pod="default/auth-deployment-54f94f4bb5-r6fmq" podUID="eb535cfe-01c3-4d17-b812-2e2aa9c42437"
Jan 22 16:50:37 minikube kubelet[1473]: E0122 16:50:37.980680    1473 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth\" with ImagePullBackOff: \"Back-off pulling image \\\"localhost:5000/spring-multi-repo-app-auth\\\"\"" pod="default/auth-deployment-54f94f4bb5-r6fmq" podUID="eb535cfe-01c3-4d17-b812-2e2aa9c42437"
Jan 22 16:50:50 minikube kubelet[1473]: E0122 16:50:50.978152    1473 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth\" with ImagePullBackOff: \"Back-off pulling image \\\"localhost:5000/spring-multi-repo-app-auth\\\"\"" pod="default/auth-deployment-54f94f4bb5-r6fmq" podUID="eb535cfe-01c3-4d17-b812-2e2aa9c42437"
Jan 22 16:51:05 minikube kubelet[1473]: E0122 16:51:05.981423    1473 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth\" with ImagePullBackOff: \"Back-off pulling image \\\"localhost:5000/spring-multi-repo-app-auth\\\"\"" pod="default/auth-deployment-54f94f4bb5-r6fmq" podUID="eb535cfe-01c3-4d17-b812-2e2aa9c42437"
Jan 22 16:51:17 minikube kubelet[1473]: E0122 16:51:17.979960    1473 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth\" with ImagePullBackOff: \"Back-off pulling image \\\"localhost:5000/spring-multi-repo-app-auth\\\"\"" pod="default/auth-deployment-54f94f4bb5-r6fmq" podUID="eb535cfe-01c3-4d17-b812-2e2aa9c42437"
Jan 22 16:51:32 minikube kubelet[1473]: E0122 16:51:32.003754    1473 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused" image="localhost:5000/spring-multi-repo-app-auth:latest"
Jan 22 16:51:32 minikube kubelet[1473]: E0122 16:51:32.003923    1473 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: Get \"http://localhost:5000/v2/\": dial tcp 127.0.0.1:5000: connect: connection refused" image="localhost:5000/spring-multi-repo-app-auth:latest"
Jan 22 16:51:32 minikube kubelet[1473]: E0122 16:51:32.004376    1473 kuberuntime_manager.go:1256] container &Container{Name:auth,Image:localhost:5000/spring-multi-repo-app-auth,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qgn6f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod auth-deployment-54f94f4bb5-r6fmq_default(eb535cfe-01c3-4d17-b812-2e2aa9c42437): ErrImagePull: Error response from daemon: Get "http://localhost:5000/v2/": dial tcp 127.0.0.1:5000: connect: connection refused
Jan 22 16:51:32 minikube kubelet[1473]: E0122 16:51:32.004549    1473 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth\" with ErrImagePull: \"Error response from daemon: Get \\\"http://localhost:5000/v2/\\\": dial tcp 127.0.0.1:5000: connect: connection refused\"" pod="default/auth-deployment-54f94f4bb5-r6fmq" podUID="eb535cfe-01c3-4d17-b812-2e2aa9c42437"
Jan 22 16:51:36 minikube kubelet[1473]: I0122 16:51:36.576473    1473 topology_manager.go:215] "Topology Admit Handler" podUID="49d29414-38dd-4713-894e-0edf46a51053" podNamespace="default" podName="auth-deployment-97d67ff76-64xkq"
Jan 22 16:51:36 minikube kubelet[1473]: I0122 16:51:36.681903    1473 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-694qc\" (UniqueName: \"kubernetes.io/projected/49d29414-38dd-4713-894e-0edf46a51053-kube-api-access-694qc\") pod \"auth-deployment-97d67ff76-64xkq\" (UID: \"49d29414-38dd-4713-894e-0edf46a51053\") " pod="default/auth-deployment-97d67ff76-64xkq"
Jan 22 16:51:37 minikube kubelet[1473]: I0122 16:51:37.085700    1473 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="1f38fc014f8573bf014e4ca2798e3e7044a7ca58ae3e485dc4757a010822f33e"
Jan 22 16:51:45 minikube kubelet[1473]: E0122 16:51:45.975647    1473 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth\" with ImagePullBackOff: \"Back-off pulling image \\\"localhost:5000/spring-multi-repo-app-auth\\\"\"" pod="default/auth-deployment-54f94f4bb5-r6fmq" podUID="eb535cfe-01c3-4d17-b812-2e2aa9c42437"
Jan 22 16:51:46 minikube kubelet[1473]: I0122 16:51:46.247579    1473 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/auth-deployment-97d67ff76-64xkq" podStartSLOduration=2.237675504 podCreationTimestamp="2024-01-22 16:51:36 +0000 UTC" firstStartedPulling="2024-01-22 16:51:37.10680067 +0000 UTC m=+212.206430556" lastFinishedPulling="2024-01-22 16:51:45.116522632 +0000 UTC m=+220.216152601" observedRunningTime="2024-01-22 16:51:46.183868966 +0000 UTC m=+221.283498977" watchObservedRunningTime="2024-01-22 16:51:46.247397549 +0000 UTC m=+221.347027518"
Jan 22 16:51:46 minikube kubelet[1473]: I0122 16:51:46.479709    1473 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-qgn6f\" (UniqueName: \"kubernetes.io/projected/eb535cfe-01c3-4d17-b812-2e2aa9c42437-kube-api-access-qgn6f\") pod \"eb535cfe-01c3-4d17-b812-2e2aa9c42437\" (UID: \"eb535cfe-01c3-4d17-b812-2e2aa9c42437\") "
Jan 22 16:51:46 minikube kubelet[1473]: I0122 16:51:46.481204    1473 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/eb535cfe-01c3-4d17-b812-2e2aa9c42437-kube-api-access-qgn6f" (OuterVolumeSpecName: "kube-api-access-qgn6f") pod "eb535cfe-01c3-4d17-b812-2e2aa9c42437" (UID: "eb535cfe-01c3-4d17-b812-2e2aa9c42437"). InnerVolumeSpecName "kube-api-access-qgn6f". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jan 22 16:51:46 minikube kubelet[1473]: I0122 16:51:46.579897    1473 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-qgn6f\" (UniqueName: \"kubernetes.io/projected/eb535cfe-01c3-4d17-b812-2e2aa9c42437-kube-api-access-qgn6f\") on node \"minikube\" DevicePath \"\""
Jan 22 16:51:48 minikube kubelet[1473]: I0122 16:51:48.978007    1473 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="eb535cfe-01c3-4d17-b812-2e2aa9c42437" path="/var/lib/kubelet/pods/eb535cfe-01c3-4d17-b812-2e2aa9c42437/volumes"
Jan 22 16:53:04 minikube kubelet[1473]: W0122 16:53:04.976701    1473 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 22 16:53:04 minikube kubelet[1473]: W0122 16:53:04.978096    1473 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 22 16:58:04 minikube kubelet[1473]: W0122 16:58:04.977358    1473 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 22 16:58:04 minikube kubelet[1473]: W0122 16:58:04.978906    1473 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 22 17:03:04 minikube kubelet[1473]: W0122 17:03:04.937576    1473 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 22 17:03:04 minikube kubelet[1473]: W0122 17:03:04.939879    1473 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 22 17:08:04 minikube kubelet[1473]: W0122 17:08:04.929185    1473 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 22 17:08:04 minikube kubelet[1473]: W0122 17:08:04.931782    1473 machine.go:65] Cannot read vendor id correctly, set empty.

* 
* ==> storage-provisioner [8f67926b7317] <==
* I0122 16:48:08.655205       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0122 16:48:08.659331       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": x509: certificate signed by unknown authority

* 
* ==> storage-provisioner [9509d491d3b7] <==
* I0122 16:48:23.067322       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0122 16:48:23.071905       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0122 16:48:23.071968       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0122 16:48:40.480525       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0122 16:48:40.480725       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"bca17a12-d063-4064-bce5-21a7dde32f3a", APIVersion:"v1", ResourceVersion:"977", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_25365f5c-7c3a-4eb5-a920-886f358067e4 became leader
I0122 16:48:40.480850       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_25365f5c-7c3a-4eb5-a920-886f358067e4!
I0122 16:48:40.582332       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_25365f5c-7c3a-4eb5-a920-886f358067e4!

